<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Study the personalization of preference fine-tuning and RLHF algorithms. Generate synthetic preference data closing the Sim2Real gap and personalizing to real users.">
  <meta name="keywords" content="RLHF, Preference Fine-Tuning, Synthetic Data, PPO, DPO, RL, LLMs">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FSPO</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation" style="background-color: #333; color: #fff;">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" style="color: #fff;">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#abstract" style="color: #fff; border-bottom: 0px solid #fff;">
        Abstract
      </a>
      <a class="navbar-item" href="#method" style="color: #fff; border-bottom: 0px solid #fff;">
        Method Overview
      </a>
      <a class="navbar-item" href="#benchmark" style="color: #fff; border-bottom: 0px solid #fff;">
        Benchmark Tasks 
      </a>
      <a class="navbar-item" href="#simreal" style="color: #fff; border-bottom: 0px solid #fff;">
        Sim2Real
      </a>
      <a class="navbar-item" href="#results" style="color: #fff; border-bottom: 0px solid #fff;">
        Results
      </a>
      <!-- <a class="navbar-item" href="#BibTeX" style="color: #fff; border-bottom: 0px solid #fff;">
        BibTeX
      </a> -->
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FSPO: Few-Shot Preference Optimization of Synthetic Data in LLMs Elicits Effective Personalization to Real Users</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://asap7772.github.io/">Anikait Singh</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sherylhsu.com/">Sheryl Hsu</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.kylehsu.org/">Kyle Hsu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ericmitchell.ai/">Eric Mitchell</a><sup>1,4</sup>,
            </span>
            <br/>
            <span class="author-block">
              <a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://thashim.github.io/">Tatsunori Hashimoto</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://architsharma97.github.io/">Archit Sharma</a><sup>1,2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><sup>1,3*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Stanford University</span>
            <span class="author-block"><sup>2</sup>Google DeepMind</span>
            <span class="author-block"><sup>3</sup>Physical Intelligence</span>
            <span class="author-block"><sup>4</sup>OpenAI</span>
            <span class="author-block"><sup>*</sup>Equal Contribution/Advising</span>
          </div>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">
              ICML 2024
            </span> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/pdf/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2404.14367"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Asap7772/fewshot-preference-optimization"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/Asap7772/fewshot-preference-optimization"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>

</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/main_fig_v5.jpg"
        alt="Teaser image."
        class="teaser-image"/>
      <h2 class="subtitle has-text-centered">
        As language models increasingly interact with a diverse user base, it becomes important for models to generate responses 
        that align with individual user preferences. Few-Shot Preference Optimization (FSPO) is a meta-learning framework that 
        leverages the strong in-context learning capabilities of an LLM to capture the diversity of human preferences. We additionally
        study crucial design decision to allow for effective transfer from synthetic to real users through synthetic preference data.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="box" style="background-color:#FFA500">
      <h2 class="title is-3" id="contributions">Contributions</h2>
      <div class="content has-text-justified">
        <p>
          <ul>
            <li><strong>Preference Optimization Framework:</strong> We propose Few-Shot Preference Optimization (FSPO), a meta-learning framework that leverages the strong in-context learning capabilities of LLMs to capture the diversity of human preferences</li>
            <li><strong>Sim2Real (Synthetic Preference Data):</strong> We study crucial design decisions to allow for effective transfer to real human participants through synthetic preference data in a <strong>controlled human study</strong></li>
            <li><strong>Personalization Benchmark:</strong> We propose a benchmark consisting of 3 domains, where personalization can be studied: (1) Reviews, (2) Explain Like I'm X (ELIX), and (3)
              Roleplay, with effective transferability to a real human-study</li>
      </div>
    </div>
  </div>
</section>


<hr style="width:750pt; margin: auto;" />


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="box" style="background-color:#ffcccb">
        <h2 class="title is-3" id="abstract">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants 
            and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot 
            Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to 
            quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. 
            Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to 
            construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available 
            LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high 
            diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users 
            across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, 
            along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are 
            personalized to synthetic users and a 72% winrate with real human users in open-ended question answering. 
          </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Abstract. -->
  <hr>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3" id="method">Method Overview</h2>
        <div class="content has-text-justified">
          <p>
            Generally, preference fine-tuning algorithms aim to optimize a reward function that captures the preferences of a user. To do so, 
            these approaches aggregate preferences from a user into a single preference dataset, which is then used to fine-tune a language model 
            through RLHF or Preference Optimization. Can we instead learn a personalized reward function directly from a few preferences of a user?
          </p>
        </div>
        
        <h3 class="title is-4">Meta-learning motivation and objective.</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          <p>
            To instantiate this idea, we propose Few-Shot Preference Optimization (FSPO), where with just the weak additional requirement of a 
            scorer id to differentiate users, we can learn a personalized reward function for each user. In FSPO, we consider each user as a new
            task instance in meta-learning, where each user has a preference dataset comprising of prompts, preferred responses, and dispreferred
            responses. Then, leveraging the strong in-context learning capabilities of LLMs, we condition the model on N preference examples from 
            the user and fine-tune the model to maximize the likelihood of the preferred responses and minimize the likelihood of the dispreferred
            response using preference optimization loss such as DPO or IPO. This allows us to learn a personalized reward function for each user.
          </p>
        </div>

        <h3 class="title is-4">User description chain-of-thought (COT).</h3>
        <div class="content has-text-justified">
          <img src="./static/images/cot.png"
          alt="Cot image."
          class="teaser-image"/>
          <p>
            When a user description is provided (even if synthetically generated), FSPO can be reframed as a two-step prediction process, 
            as illustrated above. In the first stage, the system generates a user description based on the user’s few-shot preferences. 
            In the subsequent stage, it produces a response conditioned on the original prompt, the few-shot preferences, and the generated 
            user description. This intermediate step not only offers an interpretable summary of the few-shot preferences but also serves 
            as a better representation for reward modeling and response generation.
          </p>
        </div>


        <h3 class="title is-4">Algorithm Overview.</h3>
        <div class="content has-text-justified">
          <p>
            In all, FSPO can be easily instantiated. First, a minibatch of training users is sampled. For each user, we sample a few-shot preference
            dataset and (potentially) a user description. We then fine-tune the model on the few-shot preferences and the user description using
            a preference optimization loss such as DPO or IPO (equations 5 + 6). This allows us to learn a personalized reward function for each user. If we want to do 
            COT, we train with teacher-forcing, first predicting the loss of the user description and then conditioned on the ground truth user description
            and the few-shot preferences, predict the loss on the responses. We then update with gradient descent. We summarize FSPO in Algorithm 1. 
          </p>

          <img src="./static/images/algorithm.png"
          alt="Cot image."
          class="teaser-image"/>
        </div>


        <h3 class="title is-4">User representation through preference labels.</h3>

        <div class="content has-text-justified">
          <p>
            From an information-theoretic perspective, the few-shot binary preferences can be seen as a N-bit representation of the user, 
            representing up to 2^N different personas or reward functions. There are several ways to represent users: surveys, chat histories, 
            or other forms of interaction that reveal hidden preferences. We restrict our study to such a N-bit user representation, as such 
            a constrained representation can improve the performance when transferring reward models learned on synthetic personalities to 
            real users. We defer the study of less constrained representations to future work.
          </p>
        </div>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b> FSPO offers an effective approach to personalizing open-ended question answering, by framing personalization 
              as a meta-learning problem, conditioned on few-shot preferences from a user. Additionally, FSPO can be converted to a two-step 
              prediction problem, predicting a user description conditioned on preferences and then a response, leveraging additionally 
              inference-compute and the model's instruction-tuned prior for better performance.  We summarize the algorithm framework in Algorithm 1.
            </p>
          </div>
        </div>

        <hr>

        <h2 class="title is-3" id="benchmark">Benchmark Tasks</h2>

        <div class="content has-text-justified">
          <p>
            To evaluate the performance of FSPO in generating personalized responses, we require benchmark tasks that capture a wide range of user preferences 
            to evaluate the performance of FSPO and baselines in generating personalized responses. We propose a benchmark consisting of 3 domains, where 
            personalization can be studied: (1) <strong>Reviews</strong>, studying the generation ability of models for reviews of movies, TV shows, and books 
            that are consistent with a user’s writing style, (2) <strong>Explain Like I'm X (ELIX)</strong>: studying the generation ability of models for 
            responses that are consistent with a user’s education level, and (3) <strong>Roleplay</strong>: studying the generation ability of models for responses 
            that are consistent with a user's description, with effective transferability to a real human-study. We open source the benchmark tasks with the 
            synthetic data and the evaluation scripts for the community to use.
          </p>
        </div>
        
        <hr>

        <h2 class="title is-3" id="simreal">Sim2Real: Synthetic Preference Data Transfers to Real Users</h2>

        <div class="content has-text-justified">
          <p>
            Collecting personalized data at scale presents significant challenges, primarily due to the high cost and inherent unreliability 
            of human annotation. Curating a diverse set of users to capture the full spectrum of real-world variability further complicates 
            the process, often limiting the scope and representativeness of the data. Synthetically generating data using a LLM is a promising 
            alternative, since it can both reduce costly human data generation and annotation and streamline the data curation process. 
            But how do we go about generating such a preference data using language models in a way that transfers to real people?
          </p>

          <img src="./static/images/dom_rand.png"
              alt="Cot image."
              class="teaser-image"/>

          <p>
            Following work on task construction from Meta-Learning, we propose two design decisions to effectively encourage transfer from synthetic
            to real users: (1) <strong>Encouraging Diversity</strong> and (2) <strong>Structured Task Construction</strong>. Encouraging diversity in the
            synthetic data allows us to capture a wide range of user preferences, while structured task construction allows us to capture coherent,
            self-consistent user preferences. For diversity, we sample a wide range of prompts from human participants and data sources, and augment
            these prompts by few-shot prompting with an LLM. For responses, we either used Persona-Steering or View Conditioning to encourage diversity. 
            For structured task construction, we labeled preferences in a consistent manner, utilizing a modified version of the Alpaca Eval prompt, and
            reduced underspecification of the user description by iteratively refining the user description. We summarize the task construction process
            in the flowchart below.
          </p>

          <img src="./static/images/flowchart.png"
              alt="Cot image."
              class="teaser-image"/>
        </div>

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b> Since collecting personalized data at scale is challenging, we propose instead to generate
               diverse synthetic preference datasets that can be transferred to real humans. We study two design decisions
                to effectively encourage this transfer: (1) <strong>Encouraging Diversity</strong> and (2) <strong>Structured Task Construction</strong> 
                and discuss approaches to instantiate these design choices.
            </p>

          </div>
        </div>
        
        <hr>
        
        <h2 class="title is-3" id="theory">Results</h2>
        <div class="content has-text-justified">
        <p>
          Below, we evaluate FSPO against 4 baselines: (1) a base model generating responses without any personalization, (2) Few-shot Prompting the base model,
          (3) few-shot supervised fine-tuning (Pref-FT) based off GPO, and (4) prompting with the ground truth persona. We evaluate FSPO on the 3 benchmark tasks
          and find that FSPO outperforms all baselines in generating personalized responses. We also find that COT enables us to close the gap to the oracle method,
          where we prompt with the ground truth persona. Additionally, we run a preliminary, controlled human study using Prolific, with a data collection app, that 
          was built to collect human preferences from real human participants. 
        </p>
        <table border="1">
          <thead>
              <tr>
                  <th>Method</th>
                  <th>Winrate (%)</th>
              </tr>
          </thead>
          <tbody>
              <tr>
                  <td>Base (Llama 3.2 3B instruct)</td>
                  <td>50.0</td>
              </tr>
              <tr>
                  <td>IPO</td>
                  <td>72.4</td>
              </tr>
              <tr>
                  <td>Few-shot Prompting</td>
                  <td>63.2</td>
              </tr>
              <tr>
                  <td>Few-shot Pref-FT</td>
                  <td>62.8</td>
              </tr>
              <tr>
                  <td><strong>FSPO (ours)</strong></td>
                  <td><strong>82.6</strong></td>
              </tr>
              <tr>
                  <td><strong>FSPO + COT (ours)</strong></td>
                  <td><strong>90.3</strong></td>
              </tr>
              <tr>
                  <td>Oracle (prompt w/ g.t. persona)</td>
                  <td>90.9</td>
              </tr>
          </tbody>
        </table>
        <p><strong>Table 1:</strong> Automatic Winrates on Roleplay (1500 users)</p>
      

        <table border="1">
            <thead>
                <tr>
                    <th>Method</th>
                    <th>ELIX-easy</th>
                    <th>ELIX-hard</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Base</td>
                    <td>50.0</td>
                    <td>50.0</td>
                </tr>
                <tr>
                    <td>Few-shot Prompted</td>
                    <td>92.4</td>
                    <td>81.4</td>
                </tr>
                <tr>
                    <td>Few-shot Pref-FT</td>
                    <td>91.2</td>
                    <td>82.9</td>
                </tr>
                <tr>
                    <td><strong>FSPO (Ours)</strong></td>
                    <td><strong>97.8</strong></td>
                    <td><strong>91.8</strong></td>
                </tr>
            </tbody>
        </table>
        <p><strong>Table 2:</strong> GPT-4o Winrates on <strong>ELIX-easy</strong> and <strong>ELIX-hard</strong></p>

        <table border="1">
            <thead>
                <tr>
                    <th>Method</th>
                    <th>Trained</th>
                    <th>Interpolated</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Base (Llama 3.2 3B instruct)</td>
                    <td>50.0</td>
                    <td>50.0</td>
                </tr>
                <tr>
                    <td>Few-shot Prompted (4-shot)</td>
                    <td>66.6</td>
                    <td>61.9</td>
                </tr>
                <tr>
                    <td>Few-shot Pref-FT (4-shot)</td>
                    <td>66.5</td>
                    <td>66.1</td>
                </tr>
                <tr>
                    <td><strong>FSPO (4-shot, Ours)</strong></td>
                    <td><strong>78.4</strong></td>
                    <td><strong>71.3</strong></td>
                </tr>
                <tr>
                    <td>Few-shot Prompted (8-shot)</td>
                    <td>69.1</td>
                    <td>59.1</td>
                </tr>
                <tr>
                    <td>Few-shot Pref-FT (8-shot)</td>
                    <td>65.6</td>
                    <td>70.7</td>
                </tr>
                <tr>
                    <td><strong>FSPO (8-shot, Ours)</strong></td>
                    <td><strong>80.4</strong></td>
                    <td><strong>73.6</strong></td>
                </tr>
            </tbody>
        </table>
        <p><strong>Table 3:</strong> Review Winrates - Trained and Interpolated Users</p>

        <table border="1">
            <thead>
                <tr>
                    <th>Baseline Method</th>
                    <th>Winrate (%)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>FSPO vs Base</td>
                    <td><strong>71.2</strong></td>
                </tr>
                <tr>
                    <td>FSPO vs SFT</td>
                    <td><strong>72.3</strong></td>
                </tr>
            </tbody>
        </table>
        <p><strong>Table 4:</strong> Roleplay: Human Eval Winrates</p>
      </div>

    <div class="box" style="background-color:aliceblue">
      <div class="content has-text-justified">
        <p>
          <b>Takeaway: </b>We evaluate FSPO on the 3 tasks discussed and find an <strong>87% Alpaca Eval winrate</strong> on average in 
          generating responses that are personalized to synthetic users. COT also enables us to close the gap to the oracle method, 
          where we prompt with the ground truth persona. Additionally, we run a preliminary, controlled human study, where we find a 
          <strong>72% winrate</strong> with real human users for open-ended question answering.
        </p>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>

<hr>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> 
      @inproceedings{
        tajwar2024preference,
        title={Preference Fine-Tuning of {LLM}s Should Leverage Suboptimal, On-Policy Data},
        author={Fahim Tajwar and Anikait Singh and Archit Sharma and Rafael Rafailov and Jeff Schneider and Tengyang Xie and Stefano Ermon and Chelsea Finn and Aviral Kumar},
        booktitle={Forty-first International Conference on Machine Learning},
        year={2024},
        url={https://openreview.net/forum?id=bWNPx6t0sF}
        }
</code></pre>
</div>
</section> -->
<!-- journal   = {ICML}, -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/pdf/paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Asap7772/fewshot-preference-optimization" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <p>
        The corresponding author for this work is <a href="mailto:anikait@stanford.edu">Anikait Singh</a>.<br>
        We thank the Nerfies Team for their <a href="https://github.com/nerfies/nerfies.github.io">website template</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
